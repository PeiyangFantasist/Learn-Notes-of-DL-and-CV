{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习1：张量的基本操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 类似 numpy 的方式初始化 torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8],\n",
       "         [ 9, 10, 11, 12]],\n",
       "\n",
       "        [[ 1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8],\n",
       "         [ 9, 10, 11, 12]],\n",
       "\n",
       "        [[ 1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8],\n",
       "         [ 9, 10, 11, 12]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "x = t.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]],[[1,2,3,4],[5,6,7,8],[9,10,11,12]],[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 形状变换操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
       "        [10, 11, 12,  1,  2,  3,  4,  5,  6],\n",
       "        [ 7,  8,  9, 10, 11, 12,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7,  8,  9, 10, 11, 12]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.reshape(4,9)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `cat` 函数执行拼接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按行合并矩阵，相当于\n",
    "\\begin{pmatrix}\n",
    "y \\\\\n",
    "y \\\\\n",
    "\\end{pmatrix}\n",
    "按列合并矩阵，相当于\n",
    "\\begin{pmatrix}\n",
    "y & y \\\\\n",
    "\\end{pmatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
       "        [10, 11, 12,  1,  2,  3,  4,  5,  6],\n",
       "        [ 7,  8,  9, 10, 11, 12,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
       "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
       "        [10, 11, 12,  1,  2,  3,  4,  5,  6],\n",
       "        [ 7,  8,  9, 10, 11, 12,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7,  8,  9, 10, 11, 12]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.cat((y,y),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
       "        [10, 11, 12,  1,  2,  3,  4,  5,  6, 10, 11, 12,  1,  2,  3,  4,  5,  6],\n",
       "        [ 7,  8,  9, 10, 11, 12,  1,  2,  3,  7,  8,  9, 10, 11, 12,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7,  8,  9, 10, 11, 12,  4,  5,  6,  7,  8,  9, 10, 11, 12]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.cat((y,y), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 广播机制\n",
    "- 广播机制是指不同形状的张量之间的算术运算的执行方式。\n",
    "- 即适当复制元素的行或列使得两个张量的形状相同后，再按元素运算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 索引\n",
    "1. `:` 表示范围\n",
    "2. `::` 表示步长"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
       "        [10, 11, 12,  1,  2,  3,  4,  5,  6],\n",
       "        [ 7,  8,  9, 10, 11, 12,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7,  8,  9, 10, 11, 12]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
       "        [10, 11, 12,  1,  2,  3,  4,  5,  6],\n",
       "        [ 7,  8,  9, 10, 11, 12,  1,  2,  3]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  4,  7],\n",
       "        [10,  1,  4],\n",
       "        [ 7, 10,  1],\n",
       "        [ 4,  7, 10]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:,::3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 内存管理\n",
    "`id` 查看内存地址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
       "        [ 9, 10, 11, 12, 13, 14, 15, 16, 17],\n",
       "        [18, 19, 20, 21, 22, 23, 24, 25, 26],\n",
       "        [27, 28, 29, 30, 31, 32, 33, 34, 35]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.arange(36).reshape((4,9))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 内存指定\n",
    "\n",
    "   观察下面的结果，发现：如果使用索引的方式，那么内存是不变的，而使用 `+` 会创建新的内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = t.zeros_like(y)\n",
    "z = t.zeros_like(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(u)\n",
    "u = x + y\n",
    "id(u) == before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(z)\n",
    "z[:] = x + y\n",
    "id(z) == before\n",
    "# 使用这种方式就不会创建新的内存！\n",
    "# 若后续不需使用y，则可以使用 y[:] = y + x 来减少内存开销"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面给出两种节省内存的方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Before = id(x)\n",
    "x += y\n",
    "id(x) == Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Before = id(x)\n",
    "t.add(x,y,out=x)\n",
    "id(x) == Before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy 与 Tensor 的转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.]\n",
      "tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]\n",
      "tensor([3., 3., 3., 3., 3.]) [3. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "# Tensor to Numpy (共享内存)\n",
    "import numpy as np\n",
    "a = t.ones(5)\n",
    "b = a.numpy()\n",
    "print(a, b)\n",
    "a += 1\n",
    "print(a, b)\n",
    "b += 1\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Numpy to Tensor (共享内存)\n",
    "a = np.ones(5)\n",
    "b = t.from_numpy(a)\n",
    "print(a, b)\n",
    "a += 1\n",
    "print(a, b)\n",
    "b += 1\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n",
      "[4. 4. 4. 4. 4.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n",
      "[4. 4. 4. 4. 4.] tensor([117., 117., 117., 117., 117.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 用 torch.tensor() 直接转换，就会创建新内存\n",
    "c = t.tensor(a)\n",
    "print(a, c)\n",
    "a += 1\n",
    "print(a, c)\n",
    "c += 114\n",
    "print(a, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 GPU 计算\n",
    "- `to()` 函数可以实现数据的设备转换（CPU/GPU）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.cuda.is_available() and t.cpu.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  5,  8, 11, 14, 17, 20, 23, 26],\n",
       "        [29, 32, 35, 14, 17, 20, 23, 26, 29],\n",
       "        [32, 35, 38, 41, 44, 47, 26, 29, 32],\n",
       "        [35, 38, 41, 44, 47, 50, 53, 56, 59]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = t.device(\"cuda\")\n",
    "x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  5,  8, 11, 14, 17, 20, 23, 26],\n",
       "        [29, 32, 35, 14, 17, 20, 23, 26, 29],\n",
       "        [32, 35, 38, 41, 44, 47, 26, 29, 32],\n",
       "        [35, 38, 41, 44, 47, 50, 53, 56, 59]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性代数操作\n",
    "1. 按轴求和\n",
    "   \n",
    "   如何理解 `axis` 这个量，可以直接从写法上理解。比如对于 \n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "0 & 1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 & 7 \\\\\n",
    "8 & 9 & 10 & 11\n",
    "\\end{array}\n",
    "\\right] \\\\\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "12 & 13 & 14 & 15 \\\\\n",
    "16 & 17 & 18 & 19 \\\\\n",
    "20 & 21 & 22 & 23\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$ \n",
    "去掉最外层括号，发现是两个矩阵，这就代表 `axis=0`，去掉第一层括号，发现是三个向量，这就代表 `axis=1`，去掉第二层括号，发现是四个数，这就代表 `axis=2`。\n",
    "   \n",
    "因此在求和的时候，`axis = 0` 就代表去掉最外层括号，计算第一层元素的张量和。`axis = n` 就代表去掉第 `n + 1` 层括号，计算第 `n + 1` 层元素的张量和。\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5, 19, 33, 14]) tensor([13, 38, 20])\n"
     ]
    }
   ],
   "source": [
    "p = t.tensor([[1,4,6,2],\n",
    "          [4,6,24,4],\n",
    "          [0,9,3,8]])\n",
    "print(p.sum(axis=0), p.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = t.arange(2*3*4).reshape(2,3,4)\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axis = 0 tensor([[12, 14, 16, 18],\n",
      "        [20, 22, 24, 26],\n",
      "        [28, 30, 32, 34]])\n",
      "axis = 1 tensor([[12, 15, 18, 21],\n",
      "        [48, 51, 54, 57]])\n",
      "axis = 2 tensor([[ 6, 22, 38],\n",
      "        [54, 70, 86]])\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 3):\n",
    "    print(f'axis = {i}',q.sum(axis=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axis = 0 tensor([[[12, 14, 16, 18],\n",
      "         [20, 22, 24, 26],\n",
      "         [28, 30, 32, 34]]])\n",
      "axis = 1 tensor([[[12, 15, 18, 21]],\n",
      "\n",
      "        [[48, 51, 54, 57]]])\n",
      "axis = 2 tensor([[[ 6],\n",
      "         [22],\n",
      "         [38]],\n",
      "\n",
      "        [[54],\n",
      "         [70],\n",
      "         [86]]])\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 3):\n",
    "    print(f'axis = {i}',q.sum(axis=i,keepdims=True)) # 保留维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 范数\n",
    "   1. $ L_2 $ 范数：`norm(p=2)`\n",
    "   2. $ L_1 $ 范数：`abs().sum()`\n",
    "   3. $ Forbenius $ 范数：`norm(p='fro')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.5895)\n",
      "tensor(35.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(26.5895)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = t.tensor([[1.,0,0],\n",
    "          [0,25,0],\n",
    "          [0,0,9]])\n",
    "print(t.norm(matrix, p=2))\n",
    "print(t.abs((matrix).sum()))\n",
    "t.norm(matrix, p='fro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
