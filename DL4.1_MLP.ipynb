{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习4：多层感知机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 隐藏层与激活函数\n",
    "如果隐藏层只采用线性映射，那么多层隐藏层就等价于一个线性模型。因此要使用非线性激活函数 $\\sigma$。\n",
    "$$\n",
    "H = \\sigma(XW_h + b_h) \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则化（也许叫“约束化”更好）\n",
    "#### 权重衰减\n",
    "较小的权重值意味着模型的复杂度降低，模型的拟合能力相对减弱。为了避免过拟合，我们可以对权重 $\\mathbf{w}$ 进行约束，即在损失函数中加入惩罚项：\n",
    "$$\n",
    "L(\\mathbf{w},b)+\\frac{\\lambda}{2}||\\mathbf{w}||^2\n",
    "$$\n",
    "对惩罚项求梯度会得到 $\\lambda w_i$，这个值与当前参数相关，因此很难将参数置为0.相比之下，L1范数的惩罚项更容易将参数置为0.（因为是在加减一个固定值）\n",
    "#### 暂退法\n",
    "暂退法通过加入噪声改变神经网络的结构，从而避免过拟合，具体思路是定义丢弃概率 $p$：对于每个隐藏层的每个节点，以概率 $p$ 丢弃该节点。\n",
    "\n",
    "假设原输出为 $h$，则暂退后的输出期望 $E'=(1-p)h$\n",
    "\n",
    "为了保持期望不变，需要对每个节点的输出都除以 $1-p$：\n",
    "即用 $h'$ 代替原输出 $h$：\n",
    "$$\n",
    "h'=\\begin{cases}\n",
    "0 & \\text{ 概率为 } p \\\\\n",
    "\\frac{h}{1-p} & \\text{ 概率为 } 1-p\n",
    "\\end{cases}\n",
    "\n",
    "$$\n",
    "**注意：** 暂退法并不需要多次训练，它在前向传播的过程中完成，因此那些被丢弃（输出为0）的神经元不会对梯度计算产生贡献，从而影响模型的参数更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度学习训练流程：前向计算和反向传播\n",
    "**前向计算**：即计算神经网络每一层的输出，对应数据输入，计算输出，调用激活函数，进行正则化，最后计算损失函数。\n",
    "\n",
    "**反向传播**：即计算损失函数对每一层的参数的偏导数，从而更新参数。（梯度优化）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度数值问题\n",
    "根据链式法则，对于单隐藏层的MLP，其梯度计算式如下：\n",
    "$$\n",
    "\\partial J / \\partial w_{1} = \\frac{\\partial J}{\\partial z} \\frac{\\partial z}{\\partial w_{1}} + \\frac{\\partial J}{\\partial s} \\frac{\\partial s}{\\partial w_1}\n",
    "$$\n",
    "其中，$L$ 为损失函数，$s$ 为惩罚，$z$ 为隐藏层的输出。如果层数太多（由于连乘）可能会导致梯度消失或梯度爆炸。即：\n",
    "- 梯度消失：当层数太多时，梯度会变得非常小，导致参数更新非常慢。\n",
    "- 梯度爆炸：当层数太多时，梯度会变得非常大，导致数值溢出和参数更新不稳定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数值稳定性"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
