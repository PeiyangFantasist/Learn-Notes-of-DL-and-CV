{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习2：梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义\n",
    "对于n维向量 $X$ 和 $R^n$ 上的函数 $f(X)$，梯度定义为 $\\nabla f(X) = \\frac{\\partial f}{\\partial X} $\n",
    "\n",
    "在理论中，我们要时刻注意梯度到底是什么：矩阵的梯度实质上就是函数族的梯度，可以通过计算每一个子函数的梯度间接得到。或者用我们的话来说，标量变量or向量变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 理论\n",
    "- 对 $A_{m × n}$ 都有 $\\nabla (Ax) = A$ （$Ax$ 是超平面方程）\n",
    "  \n",
    "   $ \\nabla Ax $ 实质上就是雅可比矩阵。$f_i = \\sum_j a_{ij}x_j$ 故 $ J_{ij} = \\frac{\\partial f_i}{\\partial x_j} = a_{ij}$ ，可得雅可比矩阵就是 $A$\n",
    "\n",
    "- 对 $A_{n × m}$ 都有 $\\nabla (x^TA) = A^T$\n",
    "\n",
    "   这里按列分块：$g_i=\\sum_j a_{ji}x_j$ 故 $J_{ij}^T = \\frac{\\partial g_i}{\\partial x_j} = a_{ij}$ ，可得 $J = A^T$\n",
    "  \n",
    "- 对 $A_{n × n}$ 都有 $\\nabla (x^TAx) = (A+A^T)x$\n",
    "   \n",
    "   记该二次型为 $f$，则 $f_{ij} = a_{ij}x_ix_j$ 即 $f=\\sum_i\\sum_j a_{ij}x_ix_j$ ，而 $(\\nabla f)_k =\\sum_i\\sum_j\\frac{ \\partial  a_{ij}x_ix_j}{\\partial {x_k}} = \n",
    "   \\sum_i a_{ik} x_i + \\sum_j a_{kj} x_j = g_k + f_k$ \n",
    "   \n",
    "   因此 $\\nabla f = (g_1+f_1,...,g_n+f_n)^T = (A+A^T)x$\n",
    "\n",
    "- $\\nabla \\lVert \\mathbf{x} \\rVert^2=\\nabla\\mathbf{x}^T\\mathbf{x}=2\\mathbf{x}$\n",
    "\n",
    "   由上式，$LHS=\\nabla \\mathbf{x^T} E \\mathbf{x}  = Middle = 2Ex = 2x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 自动求导\n",
    "原理：\n",
    "- 计算图的概念\n",
    "- 正向传播\n",
    "- 反向传播\n",
    "  \n",
    "  空间复杂度小\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以 $y = 2\\mathbf{x}^T \\mathbf{x}$ 为例：\n",
    "\n",
    "0. 数学计算\n",
    "   \n",
    "   $y = 2 (x_1^2 + x_2^2 + x_3^2 + x_4^2) $\n",
    "\n",
    "   $\\nabla y = 4(x_1, ... ,x_4)$\n",
    "1. 内存指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([1.,1,4,5],requires_grad=True)\n",
    "x.grad == None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 原函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 2*torch.dot(x,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.,  4., 16., 20.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward() # 反向计算求导\n",
    "x.grad # y在x（给定）处的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 清理内存\n",
    "\n",
    "   从这里可以看出 `grad` 的值是累加的，因此需要手动清理内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.,  5., 17., 21.])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x.sum()\n",
    "z.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0.])\n",
      "tensor([1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "\n",
    "print(x.grad)\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 分离计算\n",
    "   \n",
    "   人为地控制计算的流向\n",
    "\n",
    "   - `detach()` 用于创建一个新的张量，这个张量与原张量共享数据内存，但与计算图分离。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Graph示例\n",
    "    ```\n",
    "    z -->x\n",
    "    \\-->u-->x^2\n",
    "    ```\n",
    "- 数学推导\n",
    "\n",
    "  若 `z-->u` 被分离：\n",
    "\n",
    "  $z.sum = x_1 + x_2 + 16x_3 + 25x_4$\n",
    "\n",
    "  $\\nabla z.sum = (1,1,16,25)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  1., 16., 25.], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 4., 5.], requires_grad=True)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "print(y)\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  1., 16., 25.])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.sum().backward()\n",
    "x.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
