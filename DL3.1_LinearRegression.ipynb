{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习3.1：线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 理论\n",
    "1. 模型原理\n",
    "   \n",
    "   设 $X$ 为特征，$y$ 为标签，预测值 $\\hat{y}$ 由 $wX + b$ 给出\n",
    "2. 损失函数\n",
    "   \n",
    "   衡量预测值与真实值的差异，这里采用 $L(w,b) = \\frac{1}{n} \\sum_{i=1}^{n}\\frac{1}{2}(wx_i+b-y_i)^2 $\n",
    "3. 显式解（解析解）\n",
    "   \n",
    "   略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机梯度下降（SGD）\n",
    "解决寻找合适的参数 $\\mathbf{w}$ 的问题，也就是数值解。（合适的参数是指损失函数最小）\n",
    "\n",
    "1. 原理\n",
    "   \n",
    "   - 假设样本为 $B$，选择一个小样本 $b$ 和初始参数 $\\mathbf{w_0}$，在 $b$ 上计算损失函数，然后计算梯度。\n",
    "   - 引入超参数 $\\eta$，表示步长，即在下一次的更新中，参数 $\\mathbf{w_0} = (w_0,w_1...w_n,b)$ 会沿着负梯度方向走 $\\eta$ “步”。这就完成了一次更新。\n",
    "   - 引入超参数 $T$，表示迭代次数。也就是总共需要走的“步”的次数。\n",
    "2. 超参数\n",
    "   \n",
    "   即训练过程中需要人为设定的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码实现的尝试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 接下来是我的尝试（从零实现）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模拟数据\n",
    "import torch as t\n",
    "\n",
    "# 参数\n",
    "u = 0.0\n",
    "w = 0.0\n",
    "W = t.tensor([w,u]).reshape(2,1)\n",
    "# 数据集，最好都用浮点数\n",
    "# 第二列的1是偏置项系数\n",
    "a = t.tensor([1.,1])\n",
    "b = t.tensor([4.,1])\n",
    "c = t.tensor([2.,1])\n",
    "d = t.tensor([6.,1])\n",
    "x = t.stack([a,b,c,d])\n",
    "y = t.tensor([4.99,17.08,8.97,25.03]).reshape(-1,1) # 列向量\n",
    "# 超参数\n",
    "eta = 0.001\n",
    "T = 1000\n",
    "'OK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-728.9100,  -56.0700])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 梯度函数\n",
    "def gradient(W,x,y):\n",
    "    # 参数\n",
    "    y_hat = x @ W\n",
    "    y_hat = y_hat.reshape(4,1)\n",
    "    return t.tensor([t.sum((y_hat - y)*x[:,0]), t.sum(y_hat - y)])\n",
    "gradient(W,x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.2134, 0.3241])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SGD 实现\n",
    "# 调参\n",
    "eta = 0.001\n",
    "T = 1000\n",
    "res = t.tensor([w,u])\n",
    "for i in range(T):\n",
    "    res = res - gradient(res,x,y)*eta\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 接下来是 DeepSeek 修改后的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0137],\n",
      "        [0.9729]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 初始化参数（直接使用Tensor，支持自动求导）\n",
    "'''\n",
    "V = t.tensor([0.0, 0.0], requires_grad=True).reshape(2, 1)  ← 这是错误的！因为对 V 进行了reshape，V就不再是叶子节点了！没有 grad 属性！\n",
    "'''\n",
    "V = t.tensor([[0.0], [0.0]], requires_grad=True)\n",
    "# 超参数\n",
    "eta = 0.01  # 调整学习率\n",
    "T = 2000     # 增加迭代次数\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(T):\n",
    "    # 前向计算\n",
    "    y_hat = x @ V\n",
    "    # 计算损失（均方误差）\n",
    "    loss = ((y_hat - y) ** 2).mean()\n",
    "    # 反向传播（自动计算梯度）\n",
    "    loss.backward()\n",
    "    # 手动更新参数（关闭梯度跟踪）\n",
    "    with t.no_grad():\n",
    "        V -= eta * V.grad\n",
    "        V.grad.zero_()  # 清零梯度\n",
    "\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 反思：\n",
    "     - 梯度计算未取平均，数值过大导致步长变大。\n",
    "     - 使用自动求导时，要注意叶子节点的属性。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
