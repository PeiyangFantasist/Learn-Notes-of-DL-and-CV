{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习3.3： Softmax 回归\n",
    "\n",
    "实质上是线性回归的推广，我们将其称为 Softmax 回归。也就是利用线性回归来实现多分类问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 独热编码与全连接层\n",
    "\n",
    "- 独热编码\n",
    "  \n",
    "   利用一个多维向量来表示一个样本的标签。对于类别，将对应的分量置为 1，其余为 0。在之后的模型中，我们通过标签 `y` 的分量来表示条件概率： $ y_i = P(y=i|X=x)$。\n",
    "- 全连接层\n",
    "\n",
    "  全连接层是最简单的神经网络层，其每一个神经元都与上一层的所有神经元相连。比如我们输入一个 $n$ 维的特征向量，输出一个 $m$ 维的特征向量，那么全连接层的参数矩阵 $W$ 就是 $m\\times n$ 的。这样的运算保留了所有的特征信息。\n",
    "\n",
    "- 与线性回归的区别\n",
    "\n",
    "  **线性回归的输出是一个标量，而 Softmax 回归的输出是一个向量。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax 函数\n",
    "为了让输出符合概率分布，我们可以使用 Softmax 函数：\n",
    "$$\n",
    "\\hat{y}_j = \\frac{\\exp(o_j)}{\\sum_{k=1}^K \\exp(o_k)}\n",
    "$$\n",
    "其中 $\\hat{y}_j$ 表示预测为第 $j$ 类的概率，$o_j$ 表示第 $j$ 类的原始输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩阵运算的思路\n",
    "$$\n",
    "\\hat{y} = \\mathrm{softmax}(Xw + b)\n",
    "$$\n",
    "其中：\n",
    "- $X$ 为 $n×d$ 矩阵，$n$ 为样本数，$d$ 为特征数\n",
    "- $w$ 为 $d×q$ 矩阵，$q$ 为类别数\n",
    "- $b$ 为 $1×q$ 矩阵，代表每种类别的偏置\n",
    "\n",
    "执行运算时，会执行广播机制。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
